# 1. Selection Sort

**Idea:**

Repeatedly select the minimum element from the unsorted part and put it in the sorted part.

**Process:**

For each index i, find the minimum in i+1â€¦nâˆ’1.

Swap it with i.

**Passes:**

n âˆ’ 1

**Time Complexity:**

*Best:* O(nÂ²)

*Avg:* O(nÂ²)

*Worst:* O(nÂ²)

**Space Complexity:**

O(1)

**Stable?:**

No (because long-distance swaps change order of equals)

**Adaptive?:**

No (even sorted arrays take full time)

**Key Insight:**

Few swaps â†’ at most n âˆ’ 1.


# 2. Bubble Sort

**Idea:**

Repeatedly compare adjacent elements and swap if out of order.
Large elements bubble to the end.

**Process:**

For each pass, scan from 0 to nâˆ’iâˆ’2.

Swap adjacent out-of-order pairs.

If no swap in a pass â†’ array is sorted â†’ stop early.

**Passes:**

Up to n âˆ’ 1, but may stop early.

**Time Complexity:**

*Best:* O(n) (optimized version detects no swaps)

*Avg:* O(nÂ²)

*Worst:* O(nÂ²)

**Space Complexity:**

O(1)

**Stable?:**

Yes (adjacent swaps maintain order of duplicates)

**Adaptive?:**

Yes (optimized version)

**Key Insight:**

Optimized bubble sort stops early on sorted arrays.


# 3. Insertion Sort

**Idea:**

Build a sorted portion on the left and insert each new element into its correct position by shifting larger elements to the right.

**Process:**

For each index i, take key = a[i].

Move left while elements are greater than key.

Insert at correct position.

**Passes:**

n âˆ’ 1

**Time Complexity:**

*Best:* O(n) (already sorted, zero shifts)

*Avg:* O(nÂ²)

*Worst:* O(nÂ²) (reverse sorted)

**Space Complexity:**

O(1)

**Stable?:**

Yes (no jumping over equals)

**Adaptive?:**

Yes (fast on nearly sorted arrays)

**Key Insight:**

Shifting is cheaper than swapping â†’ very efficient for small or nearly-sorted data.


# 4. Merge Sort

*Idea:*

Use Divide & Conquer â€” split the array into halves, sort each half recursively, then merge the sorted halves.

*Process:*

Divide array into two halves until subarrays are size 1

Recursively sort left and right halves

Merge two sorted halves using two pointers and a temp array

*Levels:*

Splitting levels â‰ˆ âŒˆlogâ‚‚(n)âŒ‰ + 1

Merging levels â‰ˆ âŒˆlogâ‚‚(n)âŒ‰

*Time Complexity:*

Best: O(n log n)

Average: O(n log n)

Worst: O(n log n)

(At every merge level, all n elements are processed once.)

*Space Complexity:*

O(n) extra space (temporary array used during merging)

*Stable?:*

Yes (using <= during merge preserves order of equal elements)

*In-place?:*

No (requires extra memory for merging)

*Adaptive?:*

No (still performs full merge even if array is sorted)

*Key Insight:*

Although subarrays get smaller, all elements are merged at every level, leading to n Ã— log n operations.

*When to Use:*

Large datasets

When stable sorting is required

When guaranteed O(n log n) worst-case time is needed

External sorting (disk-based data)

*Comparison Highlight:*

Faster and more predictable than O(nÂ²) sorts

Uses more memory than Quick Sort



# 5. QUICK SORT

1ï¸âƒ£ What is Quick Sort?

Quick Sort is a Divide and Conquer sorting algorithm that sorts an array by:

choosing a pivot

partitioning the array around the pivot

recursively sorting the left and right subarrays

2ï¸âƒ£ Core Idea (Must Remember)

Place one element (pivot) in its correct position, then solve the same problem on smaller subarrays.

Quick Sort does not try to sort everything at once.

3ï¸âƒ£ Principle Used

Divide & Conquer

Divide â†’ partition around pivot

Conquer â†’ recursively sort subarrays

Combine â†’ no explicit merge required

4ï¸âƒ£ Pivot

A pivot is a chosen element around which the array is partitioned.

Pivot selection strategies:

First element

Last element

Random element

Median of three

ğŸ‘‰ Pivot choice strongly affects performance.

5ï¸âƒ£ Partitioning (Key Operation)

Partitioning rearranges elements such that:

All elements â‰¤ pivot are on the left

All elements > pivot are on the right

Pivot ends up in its final sorted position

After partition:

Left Subarray | Pivot | Right Subarray


The pivot index is called the partition index.

6ï¸âƒ£ Recursive Structure
quickSort(low, high):
    if low >= high â†’ return
    p = partition(low, high)
    quickSort(low, p-1)
    quickSort(p+1, high)

7ï¸âƒ£ Base Case (Very Important)
if (low >= high) return;


Why?

low == high â†’ single element (already sorted)

low > high â†’ empty subarray

Partitioning can create empty subarrays, so both must stop recursion.

8ï¸âƒ£ How Quick Sort Works (Execution Flow)

quickSort() is called

Base case checked

partition() is executed

Pivot reaches correct position

Left and right recursive calls are made

Process repeats until array is sorted

9ï¸âƒ£ Time Complexity (TC)
ğŸ”¹ Partition Cost

Partition always scans the subarray once

Cost = O(n)

ğŸ”¹ Best Case â€” O(n log n)

Pivot divides array into nearly equal subarrays

Work per level = O(n)

Number of levels = log n

O(n) Ã— log n = O(n log n)

ğŸ”¹ Average Case â€” O(n log n)

Random pivot usually produces reasonably balanced splits

Most real-world cases fall here

ğŸ”¹ Worst Case â€” O(nÂ²)

Occurs when:

Pivot is always smallest or largest

Array already sorted

Partition produces:

0 elements | n-1 elements


Total work:

n + (n-1) + (n-2) + ... + 1 = O(nÂ²)

ğŸ”¹ Time Complexity Summary
Case	Time
Best	O(n log n)
Average	O(n log n)
Worst	O(nÂ²)
ğŸ”Ÿ Space Complexity (SC)

Quick Sort is in-place, but recursion uses stack space.

ğŸ”¹ What counts as space?

Only recursive quickSort() calls

partition() is temporary â†’ O(1) space

ğŸ”¹ Best / Average Case â€” O(log n)

Balanced recursion

Stack depth â‰ˆ log n

ğŸ”¹ Worst Case â€” O(n)

Skewed recursion

Each call reduces problem size by 1

ğŸ”¹ Space Complexity Summary
Case	Space
Best / Average	O(log n)
Worst	O(n)
1ï¸âƒ£1ï¸âƒ£ Stability

âŒ Not Stable

Equal elements may change relative order due to swapping

1ï¸âƒ£2ï¸âƒ£ In-place?

âœ… Yes

No extra array required

1ï¸âƒ£3ï¸âƒ£ Adaptive?

âŒ No

Does not become faster automatically for sorted arrays

Sorted input can cause worst case

1ï¸âƒ£4ï¸âƒ£ Advantages

Very fast in practice

In-place sorting

Cache-friendly

Widely used in standard libraries

1ï¸âƒ£5ï¸âƒ£ Disadvantages

Worst-case O(nÂ²)

Not stable

Performance depends on pivot choice

1ï¸âƒ£6ï¸âƒ£ How to Improve Quick Sort

Randomized pivot selection

Median-of-three pivot

Hybrid approaches (Introsort)

1ï¸âƒ£7ï¸âƒ£ Real-World Usage

std::sort() â†’ Introsort (Quick Sort + Heap Sort)

Java primitive array sort â†’ Dual-Pivot Quick Sort

Used when performance & memory efficiency matter

1ï¸âƒ£8ï¸âƒ£ Quick Sort vs Merge Sort (One-liner)

Merge Sort â†’ stable, extra space, guaranteed O(n log n)

Quick Sort â†’ in-place, faster in practice, but worst case exists

1ï¸âƒ£9ï¸âƒ£ One-Line Interview Summary (Gold)

â€œQuick Sort is an in-place divide-and-conquer algorithm that partitions the array around a pivot and recursively sorts subarrays, achieving O(n log n) average time but O(nÂ²) in the worst case.â€

2ï¸âƒ£0ï¸âƒ£ Key Takeaways (Must Remember)

Partition does the real work

Pivot choice decides performance

Recursion depth decides space complexity

Balanced recursion â†’ fast

Skewed recursion â†’ slow